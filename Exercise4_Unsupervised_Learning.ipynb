{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Fudan PRML Fall 2024 Exercise 4: Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![news](./news.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Your name and Student ID:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In this assignment, you will build a **text classification** system which is a fundamental task in the field of Natural Language Processing (NLP). More precisely, you are given a news classification task, assigning given news texts to the categories to which they belong. Unlike traditional classification tasks, **we did not provide you with any labels for this assignment, and you need to find a way to construct labels for these articles**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For this assignment you can use commonly used deep learning frameworks like PyTorch. **You can use pretrained word vectors like Glove, but not pretrained large models like BERT.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "# setup code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%env CUDA_VISIBLE_DEVICES = 1\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "经济学家吴敬琏为什么反对“不惜代价发展芯片产业”？\n",
      "颜值很高的她，一双美腿甚至可以让人忽略她的颜值\n",
      "转自常德诗人”再访桃花源“（再续心灵故乡的故事）\n",
      "虎牙拼杀四年抢得游戏直播第一股 但真正的挑战才刚开始\n",
      "如何评价许鞍华导演？她的电影为什么总能给人以触动？\n",
      "Total number of news: 83360\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'kmeans_news.pkl'\n",
    "\n",
    "all_data = None\n",
    "with open(dataset_path,'rb') as fin:\n",
    "    all_data = pickle.load(fin)\n",
    "    all_data_np = np.array(all_data)\n",
    "\n",
    "print ('\\n'.join(all_data[0:5]))\n",
    "print ('Total number of news: {}'.format(len(all_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "\n",
    "Not all data within the dataset is suitable for clustering. You might need to filter and process some of them in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of news: 83360\n",
      "\n",
      "Original samples:\n",
      "经济学家吴敬琏为什么反对“不惜代价发展芯片产业”？\n",
      "颜值很高的她，一双美腿甚至可以让人忽略她的颜值\n",
      "转自常德诗人”再访桃花源“（再续心灵故乡的故事）\n",
      "\n",
      "Cleaned samples:\n",
      "经济学家吴敬琏为什么反对 不惜代价发展芯片产业\n",
      "颜值很高的她 一双美腿甚至可以让人忽略她的颜值\n",
      "转自常德诗人 再访桃花源 再续心灵故乡的故事\n"
     ]
    }
   ],
   "source": [
    "# 2. 数据加载和清洗\n",
    "dataset_path = 'kmeans_news.pkl'\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text.lower()\n",
    "\n",
    "# 加载数据\n",
    "with open(dataset_path,'rb') as fin:\n",
    "    all_data = pickle.load(fin)\n",
    "    all_data_np = np.array(all_data)\n",
    "\n",
    "# 清洗数据\n",
    "cleaned_data = [clean_text(news) for news in all_data]\n",
    "\n",
    "print('Total number of news:', len(all_data))\n",
    "print('\\nOriginal samples:')\n",
    "print('\\n'.join(all_data[0:3]))\n",
    "print('\\nCleaned samples:')\n",
    "print('\\n'.join(cleaned_data[0:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get embeddings for the news\n",
    "\n",
    "We need to convert the news titles into some kind of numerical representation (embedding) before we can do clustering on them. Below are two ways to get embeddings for a paragraph of text:\n",
    "\n",
    "1. **Pretrained word embeddings**: You can use pretrained word embeddings like Glove to get embeddings for each word in the news, and then average them (or try some more advanced techniques) to get the news embedding.\n",
    "\n",
    "2. **General text embedding models**: You can use general text embedding models to get embedding for a sentence directly.\n",
    "\n",
    "You can choose either of them to convert the news titles into embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: scipy 1.11.4\n",
      "Uninstalling scipy-1.11.4:\n",
      "  Successfully uninstalled scipy-1.11.4\n",
      "Found existing installation: gensim 4.3.3\n",
      "Uninstalling gensim-4.3.3:\n",
      "  Successfully uninstalled gensim-4.3.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\13004\\anaconda3\\Lib\\site-packages\\~~ipy'.\n",
      "You can safely remove it manually.\n",
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\13004\\anaconda3\\Lib\\site-packages\\~ensim'.\n",
      "You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy==1.11.4\n",
      "  Using cached scipy-1.11.4-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in c:\\users\\13004\\anaconda3\\lib\\site-packages (from scipy==1.11.4) (1.26.4)\n",
      "Using cached scipy-1.11.4-cp312-cp312-win_amd64.whl (43.7 MB)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.11.4\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.3.3-cp312-cp312-win_amd64.whl.metadata (8.2 kB)\n",
      "Using cached gensim-4.3.3-cp312-cp312-win_amd64.whl (24.0 MB)\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.3.3\n",
      "Requirement already satisfied: nltk in c:\\users\\13004\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\13004\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\13004\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\13004\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\13004\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\13004\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\13004\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 1. 首先卸载可能冲突的包\n",
    "!pip uninstall -y scipy gensim\n",
    "\n",
    "# 2. 安装指定版本的 scipy\n",
    "!pip install scipy==1.11.4\n",
    "\n",
    "# 3. 安装 gensim（使用 --no-deps 避免依赖冲突）\n",
    "!pip install --no-deps gensim\n",
    "\n",
    "# 4. 安装 nltk\n",
    "!pip install nltk\n",
    "\n",
    "# 5. 重启内核后，运行以下代码\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 加载预训练的GloVe词向量\n",
    "word_vectors = api.load('glove-wiki-gigaword-100')  # 使用100维的GloVe向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (83360, 100)\n"
     ]
    }
   ],
   "source": [
    "# 4. 计算TF-IDF权重\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(cleaned_data)\n",
    "word_to_tfidf = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n",
    "\n",
    "# 5. 定义加权句子嵌入函数\n",
    "def get_weighted_sentence_embedding(sentence, tfidf_weights):\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    weighted_vectors = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in word_vectors and token in tfidf_weights:\n",
    "            weighted_vectors.append(word_vectors[token] * tfidf_weights[token])\n",
    "    \n",
    "    if not weighted_vectors:\n",
    "        return np.zeros(100)\n",
    "    \n",
    "    return np.mean(weighted_vectors, axis=0)\n",
    "\n",
    "# 6. 为所有新闻生成加权embedding\n",
    "sentence_embeddings = np.array([\n",
    "    get_weighted_sentence_embedding(news, word_to_tfidf) \n",
    "    for news in cleaned_data\n",
    "])\n",
    "\n",
    "print(f\"Embeddings shape: {sentence_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clusters = 15\n",
    "kmeans = KMeans(n_clusters=clusters, random_state=0).fit(sentence_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View samples in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 has 81722 sentences\n",
      "不愧是中国第一门神！亚冠赛再现神奇扑救，难怪里皮如此器重他！\n",
      "山背的守望\n",
      "《后西游记》之10巧计救太后（浙江人美总17册）\n",
      "尿素直接用等于是丢钱，如何正确使用？有何使用禁忌？\n",
      "现货黄金是否值得投资？\n",
      "\n",
      "Cluster 1 has 254 sentences\n",
      "英语单词拼字小游戏（16）\n",
      "领克02价格公布，14.2~19.8万与领克01又怎样的差别？\n",
      "比亚迪汉DM成爆款，11月大卖10105台，1.4升油耗，21.98万迷倒一片\n",
      "5月1号以后能不能开具17%、11%增值税发票？怎么处理报税问题？\n",
      "安阳之夜（11）：春节时的易园门口\n",
      "\n",
      "Cluster 2 has 94 sentences\n",
      "2018 MetGala现场，金大姐金小妹纷纷亮相！\n",
      "2018“权健杯”全国青少年足球邀请赛邀请函\n",
      "2018.5.8 生猪价格 5月下旬能否出现窄幅反弹行情？\n",
      "2018 Met Ball现场，网友：大牌太多看不过来！\n",
      "2018.5.8 生猪价格 5月下旬能否出现窄幅反弹行情？\n",
      "\n",
      "Cluster 3 has 333 sentences\n",
      "“王”即将回归，剧场版动画“K SEVEN STORIES”的一些情报\n",
      "fate/stay night heaven's feel 剧场版在中国上映吗？\n",
      "谁才是真“PLUS”？宋PLUS对比CS75 PLUS\n",
      "Pro-BTC华尔街分析师敦促尽管最近的集会，但现在不购买加密\n",
      "Windows 10 Build 17093都更新了哪些内容？\n",
      "\n",
      "Cluster 4 has 34 sentences\n",
      "Square 2018年第一季度比特币的营收仅20万美元\n",
      "波兰PZL-230“蝎子”攻击机，很有科幻外表的小短腿\n",
      "《无双大蛇 3》中文官网上线，170 名角色将在异世界冒险\n",
      "你，100%没见过的“绝版”94年“牡丹壹元”\n",
      "比钢还硬的铁木筷，抗菌除霉防蛀，200°高温不变形，能用一辈子\n",
      "\n",
      "Cluster 5 has 8 sentences\n",
      "GIF：米克尔门前头球顶飞，错失进球良机\n",
      "GIF：米克尔门前头球顶飞，错失进球良机\n",
      "Nvidia 人工智能是游戏的重要组成部分\n",
      "GIF：库蒂尼奥梅开二度，巴萨再扳一城\n",
      "GIF：霹雳无敌帅炸天，洛夫伦头槌扩大比分\n",
      "\n",
      "Cluster 6 has 203 sentences\n",
      "想赚的零花钱？教你一招日入60-80\n",
      "58.6亿20525万人，从学前到高等教育，农村贫困家庭助学补助全覆盖！\n",
      "46: 1火拼！公交40分钟挪不动！这周末数万人集体冒雨出动……\n",
      "那些年我去过的寺院（48）清凉寺燃灯节\n",
      "87.0%受访家长觉得取消高考特长生加分能减少“伪特长”\n",
      "\n",
      "Cluster 7 has 370 sentences\n",
      "5月10日数字货币行情分析（BTC）变盘即将来临 方向明确再动\n",
      "095、096级核潜艇还会有龟背吗？\n",
      "MSI入围赛Day4第二场：DW 0-1 KBM，KBM成功复仇DW\n",
      "快到没朋友，奔驰AMG 老板放话 800匹狂放 GT，破百无需三秒！\n",
      "投资客开始集体撤离厦门，转战水头？｜佳铖问答NO.007\n",
      "\n",
      "Cluster 8 has 72 sentences\n",
      "dnf：奶妈再次被针对，玩家竟打卢克妮坑奶妈，这姿势你们不学？\n",
      "DNF：地下城里最重的4把武器，你能举起来哪一把？\n",
      "DNF：旭旭宝宝15娜迦王首秀，顶着能量面板破2W3，二觉171亿！\n",
      "DNF：超一线职业连乌龟心脏都打不过？\n",
      "DNF：狗托？此生都不可能狗托附身的，这下可以安心睡觉了！\n",
      "\n",
      "Cluster 9 has 101 sentences\n",
      "DS 9在中国制造，在欧洲上市37万起售，法系神龙汽车崛起\n",
      "Konami 2018财年财报：连续四年实现收益增长\n",
      "上帝账号？Facebook 承认自家后门存在\n",
      "iOS 11.4悄悄加入USB限制，不解锁无法同步\n",
      "福利：Xbox One数字版游戏赠送功能全面开启，PC游戏都能送\n",
      "\n",
      "Cluster 10 has 67 sentences\n",
      "LOL：设计师居然要移除出兵前的一分钟，这个时间你在干什么？\n",
      "LOL：亚索：机器人，你若能Q中我，我认输！机器人：你不信？\n",
      "LOL：野区又出现一个无脑怪物 刷到后期就无敌你还不了解一下？\n",
      "LOL：史上第一件在测试服就被重做的AD神器！ADC玩家激动得哭了\n",
      "「LOL」拳头将重做新手教程：艾希与瑞兹终于不是新手英雄了？\n",
      "\n",
      "Cluster 11 has 38 sentences\n",
      "杠杆资金大幅加仓股曝光！江河集团买入占比高达42.01%\n",
      "中华书法高清法帖——《智永千字文》04\n",
      "5月9日12:05，天津高速路况汇总\n",
      "5月9日（08:00）甘肃省高速公路天气、路况汇总\n",
      "「早报」去弱留强，锁仓 05.08\n",
      "\n",
      "Cluster 12 has 20 sentences\n",
      "手机测评：vivo X21对比荣耀V10，没有比较就没有伤害！\n",
      "vivo y31该如何刷机？\n",
      "vivo&雷克萨斯腾冲极边之旅完美落幕，Jovi AI表现抢眼\n",
      "vivo X21为有车一族定制了AI智能化功能！连汽车大V都被深深吸引\n",
      "vivo X21为有车一族定制了AI智能化功能！连汽车大V都被深深吸引\n",
      "\n",
      "Cluster 13 has 13 sentences\n",
      "ESPN：新的OWL席位价格将上升到三到六千万美元\n",
      "重新定义“智能汽车”，MARVEL X能成为荣威的“史册传奇”吗？\n",
      "TVB《创世纪1地产风云》：叶荣添迎来第一波失败的创业潮\n",
      "重新定义“智能汽车”，MARVEL X能成为荣威的“史册传奇”吗？\n",
      "Edinburgh｜爱丁堡美食地图24小时\n",
      "\n",
      "Cluster 14 has 31 sentences\n",
      "2021，会是一本什么样的日历陪伴着你？\n",
      "2017-2018年中国啦啦操联赛武汉活力开跳\n",
      "2020～2021赛季NBA常规赛排名讨论版（西部篇）\n",
      "杨斌出席首届《2017 房地产租赁市场与资产证券化论坛》\n",
      "2020～2021赛季NBA常规赛排名讨论版（西部篇）\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_sample = True\n",
    "for i in range(clusters):\n",
    "    print(f'Cluster {i} has {np.sum(kmeans.labels_ == i)} sentences')\n",
    "    if random_sample:\n",
    "        print('\\n'.join(all_data_np[np.random.choice(np.where(kmeans.labels_ == i)[0], 5)]))\n",
    "    else:\n",
    "        print('\\n'.join(all_data_np[kmeans.labels_ == i][0:5]))\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "30172d9a6c643631a2bbd2ccafbdc7b25d01eada6cb60dcb8ec3809d296c4202"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
